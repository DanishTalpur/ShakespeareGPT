{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "\n",
        "Libraries needed for model building, data handling, tokenization, and file operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W2icOPTK4VcW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lZcl0rKxJsl8"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Download\n",
        "\n",
        "Downloads the Tiny Shakespeare text dataset for training the language model.\n",
        "Reads the dataset file and stores the text in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSz3shnZ4qHV",
        "outputId": "84b4fe53-79ec-4e1f-e41d-8f09372d01a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-23 11:05:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-12-23 11:05:52 (39.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-h9b0D0g4wpS"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSwS6FK442x7",
        "outputId": "14809b60-054e-4910-f0fb-f4cd9ba498a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vocabulary Construction and Numerical Encoding\n",
        "\n",
        "In this cell, we construct a **character-level vocabulary** by extracting all unique characters from the text dataset. Each character is mapped to a unique integer index using lookup tables (`stoi` and `itos`).  \n",
        "Encoding and decoding functions are defined to convert text into numerical form and back.  \n",
        "Finally, the entire dataset is encoded and stored as a PyTorch tensor, making it ready for training a language model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FGIOorVL45EH"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters and Device Setup\n",
        "\n",
        "This cell defines key **hyperparameters** and selects the computation device:\n",
        "\n",
        "- **`batch_size`**: Number of sequences processed in parallel (64)  \n",
        "- **`block_size`**: Length of input sequences or context window (128)  \n",
        "- **`embed_dim`**: Dimension of token embeddings (256)  \n",
        "- **`num_heads`**: Number of attention heads in the transformer (8)  \n",
        "- **`num_layers`**: Number of transformer layers (6)  \n",
        "- **`dropout`**: Dropout rate for regularization (0.2)  \n",
        "- **`lr`**: Learning rate for the optimizer (0.0003)  \n",
        "- **`device`**: Automatically uses GPU if available, otherwise CPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W2HSDERg5FxA"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "block_size = 128     # context window\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "dropout = 0.2\n",
        "lr = 3e-4\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Generation\n",
        "\n",
        "Defines a function `get_batch()` to create training batches:\n",
        "\n",
        "- Randomly selects `batch_size` starting indices from the dataset.  \n",
        "- **Input (`x`)**: sequences of length `block_size`.  \n",
        "- **Target (`y`)**: the same sequences shifted by one character (next-token prediction).  \n",
        "- Converts both tensors to the selected computation device (CPU or GPU).  \n",
        "\n",
        "This function is used to feed data to the model during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OxuApP_u5Im3"
      },
      "outputs": [],
      "source": [
        "def get_batch():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Attention Head\n",
        "\n",
        "Defines a single **self-attention head**, which is the core component of the transformer model:\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "  - Linear layers for **keys**, **queries**, and **values** of size `head_size`.  \n",
        "  - A **lower-triangular mask (`tril`)** to prevent attending to future tokens (causal attention).  \n",
        "  - **Dropout** for regularization.\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "  - Computes keys `k`, queries `q`, and values `v` from input `x`.  \n",
        "  - Calculates **attention weights**: scaled dot-product of queries and keys.  \n",
        "  - Applies **causal mask** to prevent peeking at future tokens.  \n",
        "  - Normalizes weights with **softmax** and applies **dropout**.  \n",
        "  - Outputs the **weighted sum of values**, which represents the attended information for each position.\n",
        "\n",
        "This head allows the model to focus on different parts of the input sequence when predicting the next token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6zK65QkG5LEF"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"tril\", torch.tril(torch.ones(block_size, block_size))\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "Implements **multi-head self-attention**, combining several attention heads to capture different aspects of the input:\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "  - Splits the embedding dimension across multiple heads (`head_size = embed_dim / num_heads`).  \n",
        "  - Creates a list of `SelfAttentionHead` modules.  \n",
        "  - Linear projection and dropout applied after concatenating all heads.\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "  - Passes input `x` through each attention head.  \n",
        "  - Concatenates outputs along the feature dimension.  \n",
        "  - Applies a linear projection and dropout to produce the final output.\n",
        "\n",
        "Multi-head attention allows the model to jointly attend to information from different representation subspaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bhcwbniJ5Nl6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        head_size = embed_dim // num_heads\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttentionHead(head_size) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feed-Forward Network\n",
        "\n",
        "Defines a **position-wise feed-forward network** used in transformers:\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "  - Two linear layers with **GELU activation** in between.  \n",
        "  - Expands embedding dimension to `4 * embed_dim` and projects back to `embed_dim`.  \n",
        "  - Dropout applied for regularization.\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "  - Passes input `x` through the feed-forward network.  \n",
        "\n",
        "This layer adds **non-linearity** and **feature transformation** to the model, complementing self-attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BqqvqL-I5Pfh"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer Block\n",
        "\n",
        "Defines a single **transformer block**, combining multi-head attention and feed-forward layers with residual connections:\n",
        "\n",
        "- **Components**:\n",
        "  - `LayerNorm` before attention and feed-forward layers for normalization.  \n",
        "  - `MultiHeadAttention` for capturing dependencies across the sequence.  \n",
        "  - `FeedForward` network for position-wise transformations.\n",
        "\n",
        "- **Forward Pass**:\n",
        "  - Applies layer normalization → attention → adds residual connection.  \n",
        "  - Applies layer normalization → feed-forward → adds residual connection.  \n",
        "\n",
        "This block is the **building unit** of the GPT model, enabling both contextual understanding and feature transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NiGoYTFe5Rjd"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.sa = MultiHeadAttention()\n",
        "        self.ff = FeedForward()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shakespeare GPT Model\n",
        "\n",
        "Defines the full **GPT-style language model**:\n",
        "\n",
        "- **Embeddings**:\n",
        "  - `token_emb`: Converts token indices to embedding vectors.  \n",
        "  - `pos_emb`: Adds positional information to the embeddings.\n",
        "\n",
        "- **Transformer Blocks**:\n",
        "  - Stacks `num_layers` of `Block()` modules for deep contextual representation.\n",
        "\n",
        "- **Output**:\n",
        "  - Layer normalization followed by a linear layer (`head`) to produce logits for each token in the vocabulary.  \n",
        "  - Computes **cross-entropy loss** if target tokens are provided.\n",
        "\n",
        "This model can predict the next character in a sequence and is the core architecture for training on Shakespeare text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NxUhb8o55Txp"
      },
      "outputs": [],
      "source": [
        "class ShakespeareGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block() for _ in range(num_layers)]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok = self.token_emb(idx)\n",
        "        pos = self.pos_emb(torch.arange(T, device=device))\n",
        "        x = tok + pos\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, vocab_size),\n",
        "                targets.view(-1)\n",
        "            )\n",
        "\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9cgIVVA5W0q"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "This cell trains the **ShakespeareGPT** model:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - Instantiates the model and moves it to the selected device (CPU/GPU).  \n",
        "   - Uses the **AdamW optimizer** with the specified learning rate.\n",
        "\n",
        "2. **Training Loop** (`20000` steps):\n",
        "   - Generates a batch of input (`xb`) and target (`yb`) sequences.  \n",
        "   - Computes model predictions (`logits`) and loss.  \n",
        "   - Performs **backpropagation** and updates model parameters.\n",
        "\n",
        "3. **Logging**:\n",
        "   - Prints the training loss every `1000` steps to monitor progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLnxMu385WKR",
        "outputId": "6e7ff9a0-c3e8-4eba-feed-1857235080bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0, loss 4.2956\n",
            "step 1000, loss 1.7683\n",
            "step 2000, loss 1.5360\n",
            "step 3000, loss 1.3896\n",
            "step 4000, loss 1.3241\n",
            "step 5000, loss 1.2962\n",
            "step 6000, loss 1.2731\n",
            "step 7000, loss 1.2275\n",
            "step 8000, loss 1.2636\n",
            "step 9000, loss 1.1780\n",
            "step 10000, loss 1.1941\n",
            "step 11000, loss 1.1927\n",
            "step 12000, loss 1.1687\n",
            "step 13000, loss 1.1436\n",
            "step 14000, loss 1.1325\n",
            "step 15000, loss 1.1315\n",
            "step 16000, loss 1.1025\n",
            "step 17000, loss 1.0775\n",
            "step 18000, loss 1.0636\n",
            "step 19000, loss 1.0660\n"
          ]
        }
      ],
      "source": [
        "model = ShakespeareGPT().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for step in range(20000):\n",
        "    xb, yb = get_batch()\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        print(f\"step {step}, loss {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation Function\n",
        "\n",
        "Defines a function to **generate text** from the trained model:\n",
        "\n",
        "- **`@torch.no_grad()`**: Disables gradient calculations for faster inference.  \n",
        "- **Inputs**:\n",
        "  - `start`: Initial prompt text.  \n",
        "  - `max_new_tokens`: Number of characters to generate.  \n",
        "  - `temperature`: Controls randomness; higher values produce more diverse text.\n",
        "\n",
        "- **Process**:\n",
        "  1. Encode the starting text into tensor indices.  \n",
        "  2. Iteratively predict the next token using the model:\n",
        "     - Consider only the last `block_size` tokens (context window).  \n",
        "     - Scale logits by `temperature` and sample the next token.  \n",
        "     - Append the predicted token to the sequence.  \n",
        "\n",
        "- **Output**: Decodes the generated indices back into readable text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V1J2LXkT5aP9"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, start, max_new_tokens, temperature=0.8):\n",
        "    idx = torch.tensor([encode(start)], device=device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_idx = torch.multinomial(probs, 1)\n",
        "\n",
        "        idx = torch.cat((idx, next_idx), dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Shakespeare-Style Text\n",
        "\n",
        "- Sets the model to **evaluation mode** with `model.eval()`.  \n",
        "- Uses the `generate` function to produce **600 new characters** starting from the prompt `\"I am death\\n\"`.  \n",
        "- Prints the generated text to observe the model's output in Shakespearean style.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtvYBpPqEi8r",
        "outputId": "f7792585-260b-405f-f0b2-486158afecc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am death\n",
            "To see her beauty of mine herself:\n",
            "If not, be good night, love me not, but know.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "It is mine own eyes to you at least; I am all.\n",
            "\n",
            "LUCIO:\n",
            "Brief:\n",
            "And yet you must be a gentleman in the sea:\n",
            "Pray you now, sir, in your company mouths,\n",
            "Therefore press your comfort continue.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "In all hopes your affairs?\n",
            "\n",
            "Provost:\n",
            "Advantage thee, if you will not wet the time\n",
            "Of the virtue can go through them, where he with a\n",
            "false past and garner to his father. Besides,\n",
            "He's ears him, and in his sit is lost?\n",
            "But see whether more than flattering that she would,\n",
            "That ever were she to me a\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "print(\n",
        "    generate(\n",
        "        model,\n",
        "        start=\"I am death\\n\",\n",
        "        max_new_tokens=600,\n",
        "        temperature=0.8\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model Checkpoint\n",
        "\n",
        "Saves the trained model and important metadata:\n",
        "\n",
        "- **`model_state`**: Model weights.  \n",
        "- **`stoi` / `itos`**: Character-to-index and index-to-character mappings.  \n",
        "- **`vocab_size`**: Size of the vocabulary.  \n",
        "- **`config`**: Hyperparameters used for training.  \n",
        "\n",
        "The checkpoint is saved to `\"shakespeare_gpt.pth\"` for later loading or inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3iJIgNsXEuuW"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"stoi\": stoi,\n",
        "    \"itos\": itos,\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"config\": {\n",
        "        \"block_size\": block_size,\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"num_heads\": num_heads,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout\n",
        "    }\n",
        "}, \"shakespeare_gpt.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OlJkyzDFbPT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
