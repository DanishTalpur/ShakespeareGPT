{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8q1PO23lVnL",
        "outputId": "0f83cd4d-2ea3-4523-e80a-cc61b002f6fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-18 18:58:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-12-18 18:58:04 (37.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "h3slcy_blzkG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlTD4zhhl7Q5",
        "outputId": "a66cd51f-187e-4b7c-c6de-c387c0884595"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0Y7W6admARN",
        "outputId": "419e2f8e-d3ca-4048-e67c-eb7bdb74e0bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train(\n",
        "    files=[\"input.txt\"],\n",
        "    vocab_size=5000,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "TiwkqxnVmGQR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode(\"To be, or not to be\")\n",
        "print(encoded.tokens)\n",
        "print(encoded.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsiOMVtyoLcl",
        "outputId": "d898c4e6-5705-48f0-f273-ee5f21284752"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'Ġbe', ',', 'Ġor', 'Ġnot', 'Ġto', 'Ġbe']\n",
            "[403, 309, 16, 528, 326, 292, 309]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"tokenizer\", exist_ok=True)"
      ],
      "metadata": {
        "id": "dP0GJ1vGq191"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_model(\"tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3fV_rAVqcjl",
        "outputId": "5df54b61-ff2f-4320-f8c6-949d3992263f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tokenizer/vocab.json', 'tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"tokenizer/vocab.json\",\n",
        "    \"tokenizer/merges.txt\"\n",
        ")\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp2fRysdqlXC",
        "outputId": "d7e0fa15-b6be-4c6c-eed2-6e7b4cf38105"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode(text).ids"
      ],
      "metadata": {
        "id": "0s32AaNGrNcR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encoded, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "2q-w2WlirTMR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "gtJ7lVelrT1J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPPJMmgvEVTr",
        "outputId": "0afa2522-fd5d-4e28-efe4-7232e3d93f4d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 676, 1201,   30,  203, 2347,  336, 2752,  807, 2307])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noPEGb8-Edx7",
        "outputId": "abf9aa09-f343-4698-81a5-5f5f28081f3f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([676]) the target: 1201\n",
            "when input is tensor([ 676, 1201]) the target: 30\n",
            "when input is tensor([ 676, 1201,   30]) the target: 203\n",
            "when input is tensor([ 676, 1201,   30,  203]) the target: 2347\n",
            "when input is tensor([ 676, 1201,   30,  203, 2347]) the target: 336\n",
            "when input is tensor([ 676, 1201,   30,  203, 2347,  336]) the target: 2752\n",
            "when input is tensor([ 676, 1201,   30,  203, 2347,  336, 2752]) the target: 807\n",
            "when input is tensor([ 676, 1201,   30,  203, 2347,  336, 2752,  807]) the target: 2307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch(split):\n",
        "    d = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "cuK2diSKsBuW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "vYq8PETOsEAb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MiniGPT(vocab_size, embed_dim=256)"
      ],
      "metadata": {
        "id": "jrvqG4ylsYBV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for step in range(5000):\n",
        "    x, y = get_batch(\"train\")\n",
        "\n",
        "    logits = model(x)\n",
        "    loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 500 == 0:\n",
        "        print(step, loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOMZFhjBsbLp",
        "outputId": "2541a81a-61b0-44cd-f94c-28c11fa782c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 8.746197700500488\n",
            "500 5.918015003204346\n",
            "1000 4.947602272033691\n",
            "1500 4.598130226135254\n",
            "2000 4.243202209472656\n",
            "2500 4.285006046295166\n",
            "3000 4.134488105773926\n",
            "3500 4.063146591186523\n",
            "4000 3.9839344024658203\n",
            "4500 3.9682247638702393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, prompt, max_new_tokens=200):\n",
        "    model.eval()\n",
        "    ids = tokenizer.encode(prompt).ids\n",
        "    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model(x)\n",
        "        probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
        "        next_id = torch.multinomial(probs, 1)\n",
        "        x = torch.cat((x, next_id), dim=1)\n",
        "\n",
        "    return tokenizer.decode(x[0].tolist())"
      ],
      "metadata": {
        "id": "w4c4zVDvsdpW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model, \"God have mercy on us!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH5V-DRxshFb",
        "outputId": "9ede84e4-b175-49a8-efed-7672447c08c6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "God have mercy on us! and more sudden the tribunes at the ground stir in Rome itself forswore may say, lords; butts till, I hope I may I think I know Claudio.\n",
            "Is of England are writ, it\n",
            "Ay;\n",
            "I may they live\n",
            "BENVOLIO:\n",
            "GLOUCESTER:\n",
            "\n",
            "First His to give me regre\n",
            "Ay of them one that, and way, remember them\n",
            "To London, sir, by ne'Tis beauty,\n",
            "On struck res prosperous days ago in arms;\n",
            "\n",
            "Yet,\n",
            "My father was pleased my brother died,\n",
            "Defest.\n",
            "Forgage, though that name you, it's be thakest thy world's with death us:\n",
            "Then pith; so men is become ye, orian fray himself.\n",
            "Then thus my Antisting:\n",
            "\n",
            "\n",
            "Consort brave country's home.\n",
            "d:\n",
            "That seest. Frice hand:\n",
            "As gracious lord, the victress.\n",
            "\n",
            "I never brook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"gpt_shakespeare.pt\")"
      ],
      "metadata": {
        "id": "rXm4sM5AstjK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oORhQkMSEJO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}